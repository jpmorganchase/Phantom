{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phantom Supply Chain Example\n",
    "\n",
    "This notebook will walk you through the steps of designing and running a simple Phantom\n",
    "experiment. It is based on the included ``supply-chain-1.py`` example that can be found\n",
    "in the ``envs`` directory in the Phantom repo.\n",
    "\n",
    "In this notebook we will construct the following elements:\n",
    "\n",
    "- Actors & Agents\n",
    "- Policies\n",
    "- Reward Functions\n",
    "- Environment\n",
    "\n",
    "## Experiment Goals\n",
    "\n",
    "We want to model a very simple supply chain consisting of three types of agents:\n",
    "factories, shops and customers. Our supply chain has one product that is available in\n",
    "whole units. We do not concern ourselves with prices or profits here.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/supply-chain.svg\" />\n",
    "</p>\n",
    "\n",
    "### Factory Actor\n",
    "\n",
    "The factory is an actor in the experiment. This is because, unlike customers or the\n",
    "shop, the factory does not need to take any actions - it is purely reactive.\n",
    "\n",
    "The shop can make unlimitec requests for stock to the factory. The factory holds\n",
    "unlimited stock and can dispatch unlimited stock to the shop if requested.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/supply-chain-factory.svg\" />\n",
    "</p>\n",
    "\n",
    "### Customer Agent\n",
    "\n",
    "Customers are non-learning agents. Every step they make an order to the shop for a\n",
    "variable quantity of products. We model the number of products requested with a Poisson\n",
    "random distribution. Customers receive products from the shop after making an order. We\n",
    "do not need to do anything with this when received.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/supply-chain-customer.svg\" />\n",
    "</p>\n",
    "\n",
    "### Shop Agent\n",
    "\n",
    "The shop is the only learning agent in this experiment. It can hold infinite stock and\n",
    "can request infinite stock from the factory. It receives orders from customers and\n",
    "tries to fulfil these orders as best it can.\n",
    "\n",
    "The shop takes one action each step - the request for more stock that it sends to the\n",
    "factory. The amount it requests is decided by the policy. The policy is informed by\n",
    "one observation: the amount of stock currently held by the shop.\n",
    "\n",
    "The goal is for the shop to learn a policy where it makes the right amount of stock\n",
    "requests to the factory so it can fulfil all it's orders without holding onto too much\n",
    "unecessary stock. This goal is implemented in the shop agent's reward function.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/supply-chain-shop.svg\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "First we import the libraries we require and define some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import mercury as me\n",
    "import numpy as np\n",
    "import phantom as ph\n",
    "\n",
    "NUM_EPISODE_STEPS = 100\n",
    "\n",
    "NUM_CUSTOMERS = 5\n",
    "SHOP_MAX_STOCK = 1000\n",
    "SHOP_MAX_STOCK_REQUEST = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phantom uses the ``Mercury`` library for handling the network of agents and actors and\n",
    "the message passing between them and `Ray + RLlib <https://docs.ray.io/en/master/index.html>`_\n",
    "for running and scaling the RL training.\n",
    "\n",
    "As this experiment is simple we can easily define it entirely within one file. For more\n",
    "complex, larger experiments it is recommended to split the code into multiple files,\n",
    "making use of the modularity of Phantom.\n",
    "\n",
    "Next, for each of our agent/actor types we define a new Python class that encapsulates\n",
    "all the functionality the given agent/actor needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factory Actor\n",
    "\n",
    "The Factory is the simplest to implement as it does not take actions and does not\n",
    "store state. We inherit from Mercury's ``SimpleSyncActor`` class. The ``SimpleSyncActor``\n",
    "is an actor that handles the message it receives in a synchronous order.\n",
    "\n",
    "The ``SimpleSyncActor`` class requires that we implement a ``handle_message`` method in\n",
    "our sub-class. Here we take any stock request we receive from the shop (the ``payload``\n",
    "of the message) and reflect it back to the shop as the factory will always fulfils\n",
    "stock requests.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactoryActor(me.actors.SimpleSyncActor):\n",
    "    def __init__(self, actor_id: str):\n",
    "        super().__init__(actor_id)\n",
    "\n",
    "    # The 'handle_message' method must return messages as an iterator and hence we use\n",
    "    # the 'yield' statement instead of the usual 'return' statement.\n",
    "    def handle_message(self, ctx: me.Network.Context, msg: me.Message):\n",
    "        # The factory receives stock request messages from shop agents. We\n",
    "        # simply reflect the amount of stock requested back to the shop as the\n",
    "        # factory has unlimited stock.\n",
    "        yield (msg.sender_id, [msg.payload])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Agent\n",
    "\n",
    "The implementation of the customer agent class takes more work as it stores state and\n",
    "takes actions. For any agent to be able to interact with the RLlib framework we need to\n",
    "define methods to decode actions, encode observations, compute reward functions. Our\n",
    "customer agent takes actions according to a pre-defined policy - it does not actively\n",
    "learn - and so we can use a ``FixedPolicy`` derived class to define this simple policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerPolicy(ph.FixedPolicy):\n",
    "    # The size of the order made for each customer is determined by this fixed policy.\n",
    "    def compute_action(self, obs) -> int:\n",
    "        return np.random.poisson(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the customer agent class. We make sure to set the policy to be our\n",
    "custom fixed policy.\n",
    "\n",
    "We define a custom ``decode_action`` method. This is called every step and allows the\n",
    "customer agent to make orders to the shop. We use a random number generator to create\n",
    "varying order sizes.\n",
    "\n",
    "From this method we return a ``Packet`` object. This is a simple container than contains\n",
    "``Mutators`` and ``Messages``. In this instance we are only filling it with messages --\n",
    "mutators will be covered in a later tutorial.\n",
    "\n",
    "The ``messages`` parameter of the ``Packet`` object is a mapping of recipient IDs to a\n",
    "list of message payloads. This allows multiple messages to be send to a single\n",
    "agent/actor. In our case we are sending a single message containing a numeric value\n",
    "(the order size) to the shop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerAgent(ph.Agent):\n",
    "    # We take the ID of the shop as an initialisation parameter and store it in the\n",
    "    # agents local state. It is recommended to always handle IDs this way rather than \n",
    "    # hard-coding the values.\n",
    "    def __init__(self, agent_id: str, shop_id: str):\n",
    "        super().__init__(agent_id, policy_class=CustomerPolicy)\n",
    "\n",
    "        # We need to store the shop's ID so we can send order requests to it.\n",
    "        self.shop_id: str = shop_id\n",
    "\n",
    "    def decode_action(self, ctx: me.Network.Context, action: np.ndarray):\n",
    "        # At the start of each step we generate an order with a random size to\n",
    "        # send to the shop.\n",
    "        order_size = action\n",
    "\n",
    "        # We perform this action by sending a stock request message to the factory.\n",
    "        return ph.packet.Packet(messages={self.shop_id: [order_size]})\n",
    "\n",
    "    # As before with the factory actor, we have to define a 'handle_message' method.\n",
    "    # The customer receives messages from the shop containing the products the customer\n",
    "    # requested. The customer does not need to take any action with these messages and\n",
    "    # so we return an empty iterator using 'yield from ()'.\n",
    "    def handle_message(self, ctx: me.Network.Context, msg: me.Message):\n",
    "        # The customer will receive it's order from the shop but we do not need\n",
    "        # to take any actions on it.\n",
    "        yield from ()\n",
    "\n",
    "    def compute_reward(self, ctx: me.Network.Context) -> float:\n",
    "        # The customer agent does not learn so we do not need to construct a reward\n",
    "        # function but we do need to still return a value to satisfy RLlib.\n",
    "        return 0.0\n",
    "\n",
    "    def encode_obs(self, ctx: me.Network.Context):\n",
    "        # The customer agent does not observe anything from the environment but we\n",
    "        # have to provide a value nonetheless.\n",
    "        return 0\n",
    "\n",
    "    def get_observation_space(self):\n",
    "        return gym.spaces.Discrete(1)\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return gym.spaces.Discrete(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shop Agent\n",
    "\n",
    "As the learning agent in our experiment, the shop agent is the most complex and\n",
    "introduces some new features of Phantom. As seen below, we store more local state than\n",
    "before.\n",
    "\n",
    "We keep track of sales and missed sales over two time spans: for each step (to guide the\n",
    "policy) and for each episode (for logging purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopAgent(ph.Agent):\n",
    "    def __init__(self, agent_id: str, factory_id: str):\n",
    "        super().__init__(agent_id)\n",
    "\n",
    "        # We store the ID of the factory so we can send stock requests to it.\n",
    "        self.factory_id: str = factory_id\n",
    "\n",
    "         # We keep track of how much stock the shop has...\n",
    "        self.stock: int = 0\n",
    "\n",
    "        # ...and how many sales have been made...\n",
    "        self.sales: int = 0\n",
    "\n",
    "        # ...and how many sales per step the shop has missed due to not having enough\n",
    "        # stock.\n",
    "        self.missed_sales: int = 0\n",
    "\n",
    "    # We want to keep track of how many sales and missed sales we made in the step. When\n",
    "    # messages are sent, the shop will start taking orders. So before this happens we\n",
    "    # want to reset our counters. We can do this by defining a 'pre_resolution' method.\n",
    "    # This is called directly before messages are sent across the network in each step.\n",
    "    def pre_resolution(self, ctx: me.Network.Context):\n",
    "        # At the start of each step we reset the number of missed orders to 0.\n",
    "        self.sales = 0\n",
    "        self.missed_sales = 0\n",
    "\n",
    "    # The handle_message method is logically split into two parts: handling messages\n",
    "    # from the factory and handling messages from the customer.\n",
    "    def handle_message(self, ctx: me.Network.Context, msg: me.Message):\n",
    "        if msg.sender_id == self.factory_id:\n",
    "            # Messages received from the factory contain stock.\n",
    "            self.stock = min(self.stock + msg.payload, SHOP_MAX_STOCK)\n",
    "\n",
    "            # We do not need to respond to these messages.\n",
    "            yield from ()\n",
    "        else:\n",
    "            # All other messages are from customers and contain orders.\n",
    "            amount_requested = msg.payload\n",
    "\n",
    "            if amount_requested > self.stock:\n",
    "                self.missed_sales += amount_requested - self.stock\n",
    "                stock_to_sell = self.stock\n",
    "                self.stock = 0\n",
    "            else:\n",
    "                stock_to_sell = amount_requested\n",
    "                self.stock -= amount_requested\n",
    "\n",
    "            self.sales += stock_to_sell\n",
    "\n",
    "            # Send the customer their order.\n",
    "            yield (msg.sender_id, [stock_to_sell])\n",
    "\n",
    "    # The observation we send to the policy on each step is the shop's amount of stock\n",
    "    # it currently holds. We allow this information to be sent by defining an\n",
    "    # 'encode_obs' method:\n",
    "    def encode_obs(self, ctx: me.Network.Context):\n",
    "        # We encode the shop's current stock as the observation.\n",
    "        return self.stock\n",
    "\n",
    "    # We define a 'decode_action' method for taking the action from the policy and\n",
    "    # translating it into messages to send in the environment. Here the action taken is\n",
    "    # making requests to the factory for more stock. We place the messages we want to\n",
    "    # send in a 'Packet' container.\n",
    "    def decode_action(self, ctx: me.Network.Context, action: np.ndarray):\n",
    "        # The action the shop takes is the amount of new stock to request from\n",
    "        # the factory.\n",
    "        stock_to_request = action\n",
    "\n",
    "        # We perform this action by sending a stock request message to the factory.\n",
    "        return ph.packet.Packet(messages={self.factory_id: [stock_to_request]})\n",
    "\n",
    "    # Next we define a 'compute_reward' method. Every step we calculate a reward value\n",
    "    # based on the agents current state in the environment and send it to the policy so\n",
    "    # it can learn.\n",
    "    def compute_reward(self, ctx: me.Network.Context) -> float:\n",
    "        # We reward the agent for making sales.\n",
    "        # We penalise the agent for holding onto stock and for missing orders.\n",
    "        return self.sales - self.missed_sales - self.stock * 5\n",
    "\n",
    "    # Each episode can be thought of as a completely independent trial for the\n",
    "    # environment. However creating a new environment each time with a new network,\n",
    "    # actors and agents could potentially slow our simulations down a lot. Instead we\n",
    "    # can reset our objects back to an initial state. This is done with the 'reset'\n",
    "    # method:\n",
    "    def reset(self):\n",
    "        self.stock = 0\n",
    "\n",
    "    # Finally we need to let RLlib know the sizes of our observation space and action\n",
    "    # space so it can construct the correct neural network for the agent's policy. This\n",
    "    # is done with the 'get_observation_space' and 'get_action_space' methods:\n",
    "    def get_observation_space(self):\n",
    "        return gym.spaces.Discrete(SHOP_MAX_STOCK + 1)\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return gym.spaces.Discrete(SHOP_MAX_STOCK_REQUEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "Now we have defined all our actors and agents and their behaviours we can describe how\n",
    "they will all interact by defining our environment. Phantom provides a base\n",
    "``PhantomEnv`` class that the user should create their own class and inherit from. The\n",
    "``PhantomEnv`` class provides a default set of required methods such as ``step`` which\n",
    "coordinates the evolution of the environment for each episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupplyChainEnv(ph.PhantomEnv):\n",
    "    env_name: str = \"supply-chain-v1\"\n",
    "\n",
    "    def __init__(self, n_customers: int = 5):\n",
    "        # Define actor and agent IDs\n",
    "        shop_id = \"SHOP\"\n",
    "        factory_id = \"WAREHOUSE\"\n",
    "        customer_ids = [f\"CUST{i+1}\" for i in range(n_customers)]\n",
    "\n",
    "        # Define the agents and actors by creating instances of the classes:\n",
    "        shop_agent = ShopAgent(shop_id, factory_id=factory_id)\n",
    "        factory_actor = FactoryActor(factory_id)\n",
    "\n",
    "        customer_agents = [CustomerAgent(cid, shop_id=shop_id) for cid in customer_ids]\n",
    "\n",
    "        # Accumulate all our agents and actors in one list.\n",
    "        actors = [shop_agent, factory_actor] + customer_agents\n",
    "\n",
    "        # Define Network and create connections between Actors\n",
    "        network = me.Network(me.resolvers.UnorderedResolver(), actors)\n",
    "\n",
    "        # Connect the shop to the factory\n",
    "        network.add_connection(shop_id, factory_id)\n",
    "\n",
    "        # Connect the shop to the customers\n",
    "        network.add_connections_between([shop_id], customer_ids)\n",
    "\n",
    "        # Initialise the parent 'PhantomEnv' class, passing in the network, the number\n",
    "        # of episode steps and an optional seed value:\n",
    "        super().__init__(network=network, n_steps=NUM_EPISODE_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Before we start training we add some basic metrics to help monitor the training progress.\n",
    "These will be described in more detail in the second part of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics[\"SHOP/stock\"] = ph.logging.SimpleAgentMetric(\"SHOP\", \"stock\", \"mean\")\n",
    "metrics[\"SHOP/sales\"] = ph.logging.SimpleAgentMetric(\"SHOP\", \"sales\", \"mean\")\n",
    "metrics[\"SHOP/missed_sales\"] = ph.logging.SimpleAgentMetric(\"SHOP\", \"missed_sales\", \"mean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Shop Agent Policy\n",
    "\n",
    "Training the agents is done by making use of one of RLlib's many reinforcement learning\n",
    "algorithms. Phantom provides a wrapper around RLlib that hides much of the complexity.\n",
    "\n",
    "Training in Phantom is initiated by calling the ``ph.train`` function, passing in the\n",
    "parameters of the experiment. Any items given in the ``env_config`` dictionary will be\n",
    "passed to the initialisation method of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph.train(\n",
    "    experiment_name=\"supply-chain-notebook\",\n",
    "    algorithm=\"PPO\",\n",
    "    num_workers=1,\n",
    "    num_episodes=10,\n",
    "    env_class=SupplyChainEnv,\n",
    "    env_config=dict(n_customers=NUM_CUSTOMERS),\n",
    "    metrics=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollouts\n",
    "\n",
    "Next we can test our trained policy in the simulation by performing rollouts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ph.rollout(\n",
    "    directory=\"supply-chain-notebook/LATEST\",\n",
    "    algorithm=\"PPO\",\n",
    "    num_workers=1,\n",
    "    num_repeats=10,\n",
    "    env_config={\"n_customers\": 5},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
